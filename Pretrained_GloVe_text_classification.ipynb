{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the RAW IMDB data"
      ],
      "metadata": {
        "id": "rGPhUpUV6-7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxX3NtEN6ll1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = \"C:/Users/AFA/Downloads/aclImdb\"\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the text"
      ],
      "metadata": {
        "id": "lzlBuMgvGkeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "training_samples = 200\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "yQ-zo3OIDpxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the GloVe file"
      ],
      "metadata": {
        "id": "GlaxU6TRHc5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_dir = '/Users/fchollet/Downloads/glove.6B'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "uf8mU7KAHh9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the matrix"
      ],
      "metadata": {
        "id": "9zF0valVHtuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "h-P31d_nHtED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model"
      ],
      "metadata": {
        "id": "tvNU6FlOH45g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uKoyWS5xH6U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "b4FK-yZWH8pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluation"
      ],
      "metadata": {
        "id": "pA5wgHC5Ijws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "metadata": {
        "id": "p1jt5MalIlR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}